__Project Overview: PDF & URL RAG Chatbot__

This application provides a user-friendly interface for interacting with documents (PDFs) and web content (URLs) using a Retrieval Augmented Generation (RAG) approach. Users can upload a PDF or provide a URL, ask questions, and receive answers generated by a large language model (LLM) based on the content of the provided source.

__Architecture Diagram (Textual Representation):__

<img width="4642" height="1209" alt="image" src="https://github.com/user-attachments/assets/9e13241f-d16e-49d9-a20b-130f26988eda" />


__Explanation of the Diagram:__

1. __User (Browser):__ The user interacts with the Streamlit web application through their browser, providing either a PDF file or a URL, and then asking a question.

2. __Streamlit Web App (UI Layer):__ The `app.py` file defines the Streamlit user interface, handling input fields for file uploads, URLs, and questions, as well as the "Get Answer" button.

3. __Text Extraction:__

   - If a PDF is uploaded, `PyPDF2` is used to extract text from it.
   - If a URL is provided, `requests` fetches the web page content, and `BeautifulSoup` parses the HTML to extract visible text.

4. __Text Chunking:__ The extracted text is then broken down into smaller, manageable `chunks` using `RecursiveCharacterTextSplitter`. This helps in efficient retrieval and processing by the LLM.

5. __Embedding Generation:__ Each text chunk is converted into a numerical vector (embedding) using `HuggingFaceEmbeddings` with the `sentence-transformers/all-MiniLM-L6-v2` model. These embeddings capture the semantic meaning of the text.

6. __Vector Store (Chroma DB):__ The generated embeddings are stored in a `Chroma` vector database. This database allows for efficient similarity searches.

7. __Similarity Search:__ When the user asks a question, the question is also converted into an embedding, and a similarity search is performed in the `Chroma` vector store to retrieve the most relevant text chunks (documents) from the original source.

8. __QA Chain (LangChain):__ The retrieved documents and the user's question are passed to a `LangChain` Question Answering (QA) chain. This chain orchestrates the interaction with the LLM.

9. __Language Model (HuggingFacePipeline - Flan-T5-small):__ The `LangChain` QA chain utilizes a `HuggingFacePipeline` with the `google/flan-t5-small` model. This LLM takes the context from the retrieved documents and the user's question to generate a coherent answer.

10. __Streamlit Web App (Output):__ The generated answer is then displayed back to the user on the Streamlit web application.

This architecture effectively combines retrieval (finding relevant information) and generation (creating a human-like answer) to provide accurate and context-aware responses to user queries.
